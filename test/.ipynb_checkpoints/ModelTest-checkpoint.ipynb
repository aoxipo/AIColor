{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T02:54:18.193830Z",
     "start_time": "2022-10-19T02:54:16.792298Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "from dataloader import Dataload\n",
    "from utils.plot import plot_rect\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:14:48.507485Z",
     "start_time": "2022-10-18T09:14:48.476432Z"
    },
    "code_folding": [
     3,
     11,
     26,
     35,
     93,
     106,
     131
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "def conv_block(in_channel, out_channel):\n",
    "    layer = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channel),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1, bias=False)\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "class dense_block(nn.Module):\n",
    "    def __init__(self, in_channel, growth_rate, num_layers):\n",
    "        super(dense_block, self).__init__()\n",
    "        block = []\n",
    "        channel = in_channel\n",
    "        for i in range(num_layers):\n",
    "            block.append(conv_block(channel, growth_rate))\n",
    "            channel += growth_rate\n",
    "        self.net = nn.Sequential(*block)\n",
    "    def forward(self, x):\n",
    "        for layer in self.net:\n",
    "            out = layer(x)\n",
    "            x = torch.cat((out, x), dim=1)\n",
    "        return x\n",
    "\n",
    "def transition(in_channel, out_channel):\n",
    "    trans_layer = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channel),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channel, out_channel, 1),\n",
    "        nn.AvgPool2d(2, 2)\n",
    "    )\n",
    "    return trans_layer\n",
    "\n",
    "class densenet(nn.Module):\n",
    "    def __init__(self, in_channel, num_classes, growth_rate=32, block_layers=[6, 12, 24, 16], need_return_dic = True):\n",
    "        super(densenet, self).__init__()\n",
    "        self.need_return_dict = need_return_dic\n",
    "        self.block1 = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(in_channel, 64, 7, 2, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(3, 2, padding=1)\n",
    "            )\n",
    "        self.DB1 = self._make_dense_block(64, growth_rate,num=block_layers[0])\n",
    "        self.TL1 = self._make_transition_layer(256)\n",
    "        self.DB2 = self._make_dense_block(128, growth_rate, num=block_layers[1])\n",
    "        self.TL2 = self._make_transition_layer(512)\n",
    "        self.DB3 = self._make_dense_block(256, growth_rate, num=block_layers[2])\n",
    "        self.TL3 = self._make_transition_layer(1024)\n",
    "        self.DB4 = self._make_dense_block(512, growth_rate, num=block_layers[3])\n",
    "        self.global_average = nn.Sequential(\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "        )\n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "        #self.ea = nn.Linear(1024,2)\n",
    "    def build_results(self,x):\n",
    "        return {\n",
    "            \"pred_logits\":x,\n",
    "        }\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.DB1(x)\n",
    "        x = self.TL1(x)\n",
    "        x = self.DB2(x)\n",
    "        x = self.TL2(x)\n",
    "        x = self.DB3(x)\n",
    "        x = self.TL3(x)\n",
    "        x = self.DB4(x)\n",
    "        x = self.global_average(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #print(x.size())\n",
    "        #a = self.ea(x)\n",
    "        #print(a.size())\n",
    "        x = self.classifier(x)\n",
    "        #print(x.size())\n",
    "        return self.build_results(x) if(self.need_return_dict) else x\n",
    "\n",
    "    def _make_dense_block(self,channels, growth_rate, num):\n",
    "        block = []\n",
    "        block.append(dense_block(channels, growth_rate, num))\n",
    "        channels += num * growth_rate\n",
    "\n",
    "        return nn.Sequential(*block)\n",
    "    def _make_transition_layer(self,channels):\n",
    "        block = []\n",
    "        block.append(transition(channels, channels // 2))\n",
    "        return nn.Sequential(*block)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        x = x.view(x.shape[0], -1, 4)\n",
    "        return x\n",
    "    \n",
    "class ShareMLP(MLP):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(ShareMLP, self).__init__(input_dim, hidden_dim, output_dim, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x_src = x\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = x + x_src\n",
    "        x = x.view(x.shape[0], -1, 4)\n",
    "        return x\n",
    "    \n",
    "class DenseCoord(densenet):\n",
    "    def __init__(self, in_channel, num_classes, num_queries = 25,growth_rate = 32, block_layers=[6, 12, 24, 16], need_return_dic = True):\n",
    "\n",
    "        super(DenseCoord,self).__init__(in_channel, num_classes, growth_rate=growth_rate, block_layers=block_layers,\n",
    "                                        need_return_dic = need_return_dic)\n",
    "        self.num_classes = num_classes + 1\n",
    "        self.class_embed = nn.Linear(1024, self.num_classes * num_queries)\n",
    "        self.bbox_embed = MLP(1024, 1024, 4 * num_queries, 3)\n",
    "        #self.bbox_embed = ShareMLP(1024, 1024, 4 * num_queries, 3)\n",
    "        \n",
    "    def build_results(self,x,y):\n",
    "        return {\n",
    "            \"pred_logits\":x,\n",
    "            \"pred_boxes\":y,\n",
    "        }\n",
    "    def feature(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.DB1(x)\n",
    "        x = self.TL1(x)\n",
    "        x = self.DB2(x)\n",
    "        x = self.TL2(x)\n",
    "        x = self.DB3(x)\n",
    "        x = self.TL3(x)\n",
    "        x = self.DB4(x)\n",
    "        x = self.global_average(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return x\n",
    "    def forward(self, x):\n",
    "        feature_map = self.feature(x)\n",
    "        print(feature_map.shape)\n",
    "        class_feature = self.class_embed(feature_map)\n",
    "        print(class_feature.shape)\n",
    "        outputs_class = class_feature.view(class_feature.shape[0], -1, self.num_classes)    # one-hot\n",
    "        outputs_coord = self.bbox_embed(feature_map).sigmoid()\n",
    "        print(outputs_class.shape)\n",
    "        print(outputs_coord.shape)\n",
    "        return self.build_results(outputs_class, outputs_coord) if (self.need_return_dict) else [outputs_class,outputs_coord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:14:48.603244Z",
     "start_time": "2022-10-18T09:14:48.526446Z"
    }
   },
   "outputs": [],
   "source": [
    "model = DenseCoord(3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:14:49.045345Z",
     "start_time": "2022-10-18T09:14:48.915302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024])\n",
      "torch.Size([2, 225])\n",
      "torch.Size([2, 25, 9])\n",
      "torch.Size([2, 25, 4])\n"
     ]
    }
   ],
   "source": [
    "image = torch.zeros((2,3,128,128))\n",
    "d = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:14:52.214934Z",
     "start_time": "2022-10-18T09:14:52.196480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['pred_boxes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:14:53.014733Z",
     "start_time": "2022-10-18T09:14:52.996740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25, 9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['pred_logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:22:20.142287Z",
     "start_time": "2022-10-18T09:22:20.132312Z"
    }
   },
   "outputs": [],
   "source": [
    "costCross = torch.nn.CrossEntropyLoss()\n",
    "l2 = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:18:40.948989Z",
     "start_time": "2022-10-18T09:18:40.933603Z"
    }
   },
   "outputs": [],
   "source": [
    "label = torch.zeros((2,25,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:18:41.437335Z",
     "start_time": "2022-10-18T09:18:41.427363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:37:45.070179Z",
     "start_time": "2022-10-18T09:37:45.043797Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[2, 25, 5]}, size=[1]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[2, 25, 5]}, size=[1]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "label.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:21:11.272040Z",
     "start_time": "2022-10-18T09:21:11.267037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[:,:,0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:24:55.500202Z",
     "start_time": "2022-10-18T09:24:55.492107Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = d['pred_logits'].view(-1, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-18T09:25:02.192399Z",
     "start_time": "2022-10-18T09:25:02.179924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 9])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T03:48:27.551346Z",
     "start_time": "2022-10-19T03:48:27.534348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(32, 2, kernel_size=(2, 2), stride=(2, 2))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv2d = nn.Sequential(\n",
    "            nn.Conv2d(256,64,2,2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T03:36:36.392196Z",
     "start_time": "2022-10-19T03:36:36.382217Z"
    }
   },
   "outputs": [],
   "source": [
    "image = torch.zeros([2, 256, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T04:09:47.393929Z",
     "start_time": "2022-10-19T04:09:47.372773Z"
    }
   },
   "outputs": [],
   "source": [
    "class BoxEmbed(nn.Module):\n",
    "    def __init__(self, model_in_channel, W, H, num_require = 25, coord_number = 4):\n",
    "        super(BoxEmbed,self).__init__()\n",
    "        conv_list = []\n",
    "        self.coord_number = coord_number\n",
    "        channel = model_in_channel\n",
    "        for i in range( int(channel/32) -1 ):\n",
    "            W = W/2\n",
    "            if( W > 4 ):\n",
    "                kernal_size = 2\n",
    "                stride = 2\n",
    "            else:\n",
    "                kernal_size = 1\n",
    "                stride = 1\n",
    "            if(channel > 32):\n",
    "                channel = int(channel/2)\n",
    "                in_channel = 2*channel\n",
    "                out_channel = channel\n",
    "            else:\n",
    "                in_channel = channel\n",
    "                out_channel = channel\n",
    "            conv_list.append(nn.Conv2d(in_channel,out_channel,kernal_size,stride))\n",
    "            conv_list.append(nn.BatchNorm2d(out_channel))\n",
    "            conv_list.append(nn.ReLU())\n",
    "        self.conv_block = nn.Sequential(*conv_list)\n",
    "        self.embed = nn.Conv1d(512, num_require,1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = x.view(x.shape[0],-1, self.coord_number)  # [Batch,512,4]\n",
    "        x = self.embed(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T04:10:04.338403Z",
     "start_time": "2022-10-19T04:10:04.315445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25, 4])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_image = torch.zeros((2, 128, 8, 8))\n",
    "B,C,W,H = batch_image.shape\n",
    "b = BoxEmbed(C,W,H,25,4)\n",
    "b(batch_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T04:08:38.357973Z",
     "start_time": "2022-10-19T04:08:38.348002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 4])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_image = torch.zeros((2,512,4))\n",
    "batch_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T04:09:17.094881Z",
     "start_time": "2022-10-19T04:09:17.084933Z"
    }
   },
   "outputs": [],
   "source": [
    "d = nn.Conv1d(512, 25,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T04:09:17.475372Z",
     "start_time": "2022-10-19T04:09:17.467395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25, 4])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d(batch_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T04:06:59.415692Z",
     "start_time": "2022-10-19T04:06:59.340771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 8, 8])\n",
      "torch.Size([2, 512, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1024x4 and 512x25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [140]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\DSIM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [138]\u001b[0m, in \u001b[0;36mBoxEmbed.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoord_number)  \u001b[38;5;66;03m# [Batch,512,4]\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\DSIM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\DSIM\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1024x4 and 512x25)"
     ]
    }
   ],
   "source": [
    "b(batch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T04:02:51.824686Z",
     "start_time": "2022-10-19T04:02:51.804710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 4])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.view(d.shape[0],-1,4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T06:29:53.998116Z",
     "start_time": "2022-10-19T06:29:53.952589Z"
    },
    "code_folding": [
     6,
     32
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    " \n",
    "#ResNet的基本Bottleneck类\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion=4#通道倍增数\n",
    "    def __init__(self,in_planes,planes,stride=1,downsample=None):\n",
    "        super(Bottleneck,self).__init__()\n",
    "        self.bottleneck=nn.Sequential(\n",
    "            nn.Conv2d(in_planes,planes,1,bias=False),\n",
    "            nn.BatchNorm2d(planes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(planes,planes,3,stride,1,bias=False),\n",
    "            nn.BatchNorm2d(planes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(planes,self.expansion*planes,1,bias=False),\n",
    "            nn.BatchNorm2d(self.expansion*planes),\n",
    "        )\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        self.downsample=downsample\n",
    "    def forward(self,x):\n",
    "        identity=x\n",
    "        out=self.bottleneck(x)\n",
    "        if self.downsample is not None:\n",
    "            identity=self.downsample(x)\n",
    "        out+=identity\n",
    "        out=self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self,in_channel = 3, layers = [2,2,2,2]):\n",
    "        super(FPN,self).__init__()\n",
    "        self.inplanes=64\n",
    "        #处理输入的C1模块（C1代表了RestNet的前几个卷积与池化层）\n",
    "        self.conv1=nn.Conv2d(in_channel,64,7,2,3,bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(64)\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        self.maxpool=nn.MaxPool2d(3,2,1)\n",
    "        #搭建自下而上的C2，C3，C4，C5\n",
    "        self.layer1=self._make_layer(64,layers[0])\n",
    "        self.layer2=self._make_layer(128,layers[1],2)\n",
    "        self.layer3=self._make_layer(256,layers[2],2)\n",
    "        self.layer4=self._make_layer(512,layers[3],2)\n",
    "        #对C5减少通道数，得到P5\n",
    "        self.toplayer=nn.Conv2d(2048,256,1,1,0)\n",
    "        #3x3卷积融合特征\n",
    "        self.smooth1=nn.Conv2d(256,256,3,1,1)\n",
    "        self.smooth2=nn.Conv2d(256,256,3,1,1)\n",
    "        self.smooth3=nn.Conv2d(256,256,3,1,1)\n",
    "        #横向连接，保证通道数相同\n",
    "        self.latlayer1=nn.Conv2d(1024,256,1,1,0)\n",
    "        self.latlayer2=nn.Conv2d(512,256,1,1,0)\n",
    "        self.latlayer3=nn.Conv2d(256,256,1,1,0)\n",
    "        \n",
    "    def _make_layer(self,planes,blocks,stride=1):\n",
    "        downsample=None\n",
    "        if stride!=1 or self.inplanes != Bottleneck.expansion * planes:\n",
    "            downsample=nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes,Bottleneck.expansion*planes,1,stride,bias=False),\n",
    "                nn.BatchNorm2d(Bottleneck.expansion*planes)\n",
    "            )\n",
    "        layers=[]\n",
    "        layers.append(Bottleneck(self.inplanes,planes,stride,downsample))\n",
    "        self.inplanes=planes*Bottleneck.expansion\n",
    "        for i in range(1,blocks):\n",
    "            layers.append(Bottleneck(self.inplanes,planes))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    #自上而下的采样模块\n",
    "    def _upsample_add(self,x,y):\n",
    "        _,_,H,W=y.shape\n",
    "        return F.upsample(x,size=(H,W),mode='bilinear')+y\n",
    "    def forward(self,x):\n",
    "        #自下而上\n",
    "        c1=self.maxpool(self.relu(self.bn1(self.conv1(x))))\n",
    "        c2=self.layer1(c1)\n",
    "        c3=self.layer2(c2)\n",
    "        c4=self.layer3(c3)\n",
    "        c5=self.layer4(c4)\n",
    "        #自上而下\n",
    "        p5=self.toplayer(c5)\n",
    "        p4=self._upsample_add(p5,self.latlayer1(c4))\n",
    "        p3=self._upsample_add(p4,self.latlayer2(c3))\n",
    "        p2=self._upsample_add(p3,self.latlayer3(c2))\n",
    "        #卷积的融合，平滑处理\n",
    "        p4=self.smooth1(p4)\n",
    "        p3=self.smooth2(p3)\n",
    "        p2=self.smooth3(p2)\n",
    "        return p2,p3,p4,p5\n",
    "\n",
    "class CBL(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernal_size = 3, stride = 1, padding = 1):\n",
    "        super(CBL,self).__init__()\n",
    "        self.cblblock = nn.Sequential(\n",
    "            nn.Conv2d(in_channel,out_channel,kernal_size,stride,padding),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.cblblock(x)\n",
    "\n",
    "class BoxEmbed(nn.Module):\n",
    "    def __init__(self, model_in_channel, W,num_require = 25, coord_number = 4):\n",
    "        super(BoxEmbed,self).__init__()\n",
    "        conv_list = []\n",
    "        self.coord_number = coord_number\n",
    "        channel = model_in_channel\n",
    "        for i in range( int(channel/32) -1 ):\n",
    "            W = W/2\n",
    "            if( W > 4 ):\n",
    "                kernal_size = 2\n",
    "                stride = 2\n",
    "            else:\n",
    "                kernal_size = 1\n",
    "                stride = 1\n",
    "            if(channel > 32):\n",
    "                channel = int(channel/2)\n",
    "                in_channel = 2*channel\n",
    "                out_channel = channel\n",
    "            else:\n",
    "                in_channel = channel\n",
    "                out_channel = channel\n",
    "            conv_list.append(nn.Conv2d(in_channel,out_channel,kernal_size,stride))\n",
    "            conv_list.append(nn.BatchNorm2d(out_channel))\n",
    "            conv_list.append(nn.ReLU())\n",
    "        self.conv_block = nn.Sequential(*conv_list)\n",
    "        self.embed = nn.Conv1d(512, num_require,1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = x.view(x.shape[0],-1, self.coord_number)  # [Batch,512,4]\n",
    "        x = self.embed(x)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "class MixFpn(nn.Module):\n",
    "    def __init__(self,in_channel = 3, layers = [2,2,2,2], num_class = 2, num_require = 25, need_return_dict = True):\n",
    "        super(MixFpn,self).__init__()\n",
    "        self.fpn = FPN(in_channel, layers)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(256,128,3,1,1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.hidden = 128\n",
    "        self.cbl_same = CBL(self.hidden,self.hidden)\n",
    "        self.cbl_down1 = CBL(2*self.hidden,self.hidden)\n",
    "        self.cbl_down = CBL(self.hidden,self.hidden,2,2,0)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(2*self.hidden,64,2,2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.num_require = num_require\n",
    "        self.need_return_dict = need_return_dict\n",
    "        self.softmax = nn.Linear(1024, num_require * (num_class + 1) * 2)\n",
    "        self.class_embed = nn.Linear(num_require * (num_class + 1) * 2, num_require * (num_class + 1))\n",
    "        self.box_embed1 = BoxEmbed(self.hidden*2, 32, num_require=num_require)\n",
    "        self.box_embed2 = BoxEmbed(self.hidden*2, 16, num_require=num_require)\n",
    "        self.box_embed3 = BoxEmbed(self.hidden*2, 8, num_require=num_require)\n",
    "    def _upsample(self, x, H, W):\n",
    "        return F.upsample(x,size=(H,W),mode='bilinear')\n",
    "    def _upsample_add(self,x,y):\n",
    "        _,_,H,W=y.shape\n",
    "        return self._upsample(x,H,W)+y\n",
    "    def feature(self, x):\n",
    "        p2,p3,p4,p5 = self.fpn(x)\n",
    "        B,C,H,W = p2.shape\n",
    "        \n",
    "        for i in range(int(C/self.hidden)-1):\n",
    "            p2 = self.conv1(p2)\n",
    "            p3 = self.conv1(p3)\n",
    "            p4 = self.conv1(p4)\n",
    "            p5 = self.conv1(p5)\n",
    "         \n",
    "        x1 = torch.cat([self.cbl_same(p2), self._upsample(p5,H,W)], dim=1)\n",
    "        feature1 = self.cbl_down1(x1)\n",
    "        x2 = torch.cat([p3, self.cbl_down(feature1)],dim=1)\n",
    "        feature2 = self.cbl_down1(x2)\n",
    "        x3 = torch.cat([p4, self.cbl_down(feature2)],dim=1)\n",
    "        return x1,x2,x3\n",
    "    def build_results(self,x,y):\n",
    "        return {\n",
    "            \"pred_logits\":x,\n",
    "            \"pred_boxes\":y,\n",
    "        }\n",
    "    def forward(self,x):\n",
    "        x = self.feature(x)\n",
    "        box_coord_1 = self.box_embed1(x[0])\n",
    "        box_coord_2 = self.box_embed2(x[1])\n",
    "        box_coord_3 = self.box_embed3(x[2])\n",
    "        pred_coord = (box_coord_1 * box_coord_2 * box_coord_3).sigmoid()\n",
    "        x = self.conv(x[-1])\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.softmax(x)\n",
    "        pred_class = self.class_embed(x)\n",
    "        return self.build_results(pred_class,pred_coord) if(self.need_return_dict) else [pred_class,pred_coord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T06:29:54.390933Z",
     "start_time": "2022-10-19T06:29:54.382955Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_image = torch.zeros(2,3,128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T06:29:57.812728Z",
     "start_time": "2022-10-19T06:29:57.697485Z"
    }
   },
   "outputs": [],
   "source": [
    "m = MixFpn(3,[2,2,2,2],8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T06:29:58.447185Z",
     "start_time": "2022-10-19T06:29:58.320525Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\DSIM\\lib\\site-packages\\torch\\nn\\functional.py:3704: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "d = m(batch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T06:29:59.247987Z",
     "start_time": "2022-10-19T06:29:59.231972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_logits': tensor([[ 4.0452e-01,  4.0035e-01,  5.7632e-01,  1.5997e-01,  1.3133e-01,\n",
       "          -2.3799e-01,  1.3313e-01, -1.5027e-01, -3.7701e-01, -7.0471e-02,\n",
       "          -2.0735e-01,  3.1206e-01,  3.0096e-02, -7.2557e-02,  2.2506e-01,\n",
       "          -1.7077e-01,  1.0950e-01,  3.4032e-01,  1.2843e-01, -1.1341e-01,\n",
       "           9.0018e-02,  1.9510e-01,  6.1548e-02, -1.8315e-01, -1.2946e-01,\n",
       "          -7.0517e-02,  1.6340e-01,  3.9783e-01,  3.3278e-01, -8.3161e-02,\n",
       "          -9.4468e-02,  3.3800e-02, -1.3770e-01,  2.2086e-01,  1.5825e-01,\n",
       "           1.5790e-01,  3.5834e-01,  2.5845e-01, -3.1832e-01,  1.9514e-01,\n",
       "           2.2260e-01, -3.7083e-01, -4.5805e-01, -2.8093e-03, -8.9936e-02,\n",
       "          -1.6657e-02, -1.8714e-01, -9.7226e-02,  1.2922e-01,  5.5807e-02,\n",
       "          -6.3892e-02,  1.0089e-01, -7.8274e-02,  2.1929e-02,  5.3395e-01,\n",
       "           1.7657e-01, -2.8625e-01, -4.5904e-02, -5.3479e-01, -1.4283e-01,\n",
       "          -1.5624e-03,  1.0799e-01,  4.6423e-01,  1.9305e-01, -1.9696e-01,\n",
       "          -2.5663e-01,  3.2696e-01,  5.0250e-02,  3.9706e-01,  1.4484e-01,\n",
       "           1.0189e-01, -1.7885e-01, -5.2135e-01, -2.2980e-02, -1.2584e-01,\n",
       "           2.4603e-01,  1.9162e-01,  1.1357e-01,  3.8967e-02, -1.3164e-01,\n",
       "           2.5938e-01,  2.0989e-01, -6.6325e-02,  3.1560e-01, -8.7636e-02,\n",
       "          -3.1452e-01, -1.2608e-01,  1.5065e-01,  5.5979e-01,  2.3979e-02,\n",
       "           1.9262e-01, -1.0185e-01,  1.6800e-01,  2.7170e-01,  2.2030e-01,\n",
       "          -1.0784e-01, -2.3142e-01,  8.0370e-02,  2.3275e-01,  1.5560e-01,\n",
       "          -5.8212e-02, -4.6966e-02, -8.8897e-02,  6.3531e-01, -5.7413e-02,\n",
       "          -4.4482e-02, -2.1070e-01,  2.1130e-02, -2.0953e-02,  2.9690e-01,\n",
       "          -2.3055e-01, -1.1976e-01,  3.5724e-01, -3.7660e-01, -3.1279e-01,\n",
       "          -9.4088e-02,  4.0741e-02, -1.2041e-01,  1.1146e-02, -1.1843e-01,\n",
       "          -1.1988e-01, -2.0048e-01,  3.7020e-01, -5.4336e-03, -2.1060e-01,\n",
       "          -3.5244e-01,  1.9725e-01, -2.8410e-01, -2.1667e-02,  1.1906e-01,\n",
       "          -1.6232e-01,  1.4037e-01, -2.6254e-01, -1.9619e-02,  1.3825e-01,\n",
       "          -1.0288e-01, -3.1256e-01,  2.2326e-01,  4.7886e-01,  2.3181e-01,\n",
       "           3.2818e-01, -1.9606e-02,  8.2409e-02, -4.2025e-01,  3.6256e-02,\n",
       "           5.5572e-02, -1.5449e-01, -4.0418e-02, -6.1560e-02, -1.8169e-01,\n",
       "           2.4517e-01, -3.7002e-02, -3.6960e-01, -8.8844e-02, -2.3780e-01,\n",
       "           1.6417e-01,  1.0817e-01, -1.7400e-01,  2.7917e-01, -2.2443e-01,\n",
       "          -6.6816e-02, -2.3988e-01,  1.5288e-01, -2.1164e-02, -8.4117e-02,\n",
       "           2.0744e-02, -1.7678e-01,  9.1154e-02, -7.9493e-02,  2.3486e-01,\n",
       "           9.4412e-02,  5.9954e-02, -1.3508e-01,  2.5998e-01,  4.7383e-01,\n",
       "           1.8552e-01,  4.0855e-01, -3.3529e-01, -1.3373e-01, -2.2181e-01,\n",
       "          -2.9527e-01, -3.4575e-01, -3.4687e-01,  1.6851e-01,  3.5043e-02,\n",
       "           4.4696e-02,  3.9911e-01, -4.3358e-01,  6.4146e-02, -2.2804e-02,\n",
       "           6.6244e-02,  9.4090e-02, -3.7099e-01,  4.2853e-01,  5.4155e-04,\n",
       "          -2.1277e-01, -1.7115e-01,  2.4853e-01, -3.4741e-01,  6.0674e-02,\n",
       "          -9.0868e-02,  4.6684e-01, -1.0172e-02,  2.2984e-02,  2.6720e-01,\n",
       "          -3.6501e-01,  1.1859e-01,  2.6614e-01,  2.3305e-01, -7.0079e-02,\n",
       "           1.9824e-01,  1.5617e-01,  2.7291e-01, -1.5273e-01,  8.7938e-02,\n",
       "           1.3764e-01, -1.0097e-01,  7.3936e-02,  6.1457e-03, -4.9488e-02,\n",
       "           1.1659e-01, -1.7241e-01,  1.4152e-01, -1.5163e-01,  1.0425e-01],\n",
       "         [ 4.0452e-01,  4.0035e-01,  5.7632e-01,  1.5997e-01,  1.3133e-01,\n",
       "          -2.3799e-01,  1.3313e-01, -1.5027e-01, -3.7701e-01, -7.0471e-02,\n",
       "          -2.0735e-01,  3.1206e-01,  3.0096e-02, -7.2557e-02,  2.2506e-01,\n",
       "          -1.7077e-01,  1.0950e-01,  3.4032e-01,  1.2843e-01, -1.1341e-01,\n",
       "           9.0018e-02,  1.9510e-01,  6.1548e-02, -1.8315e-01, -1.2946e-01,\n",
       "          -7.0517e-02,  1.6340e-01,  3.9783e-01,  3.3278e-01, -8.3161e-02,\n",
       "          -9.4468e-02,  3.3800e-02, -1.3770e-01,  2.2086e-01,  1.5825e-01,\n",
       "           1.5790e-01,  3.5834e-01,  2.5845e-01, -3.1832e-01,  1.9514e-01,\n",
       "           2.2260e-01, -3.7083e-01, -4.5805e-01, -2.8093e-03, -8.9936e-02,\n",
       "          -1.6657e-02, -1.8714e-01, -9.7226e-02,  1.2922e-01,  5.5807e-02,\n",
       "          -6.3892e-02,  1.0089e-01, -7.8274e-02,  2.1929e-02,  5.3395e-01,\n",
       "           1.7657e-01, -2.8625e-01, -4.5904e-02, -5.3479e-01, -1.4283e-01,\n",
       "          -1.5624e-03,  1.0799e-01,  4.6423e-01,  1.9305e-01, -1.9696e-01,\n",
       "          -2.5663e-01,  3.2696e-01,  5.0250e-02,  3.9706e-01,  1.4484e-01,\n",
       "           1.0189e-01, -1.7885e-01, -5.2135e-01, -2.2980e-02, -1.2584e-01,\n",
       "           2.4603e-01,  1.9162e-01,  1.1357e-01,  3.8967e-02, -1.3164e-01,\n",
       "           2.5938e-01,  2.0989e-01, -6.6325e-02,  3.1560e-01, -8.7636e-02,\n",
       "          -3.1452e-01, -1.2608e-01,  1.5065e-01,  5.5979e-01,  2.3979e-02,\n",
       "           1.9262e-01, -1.0185e-01,  1.6800e-01,  2.7170e-01,  2.2030e-01,\n",
       "          -1.0784e-01, -2.3142e-01,  8.0370e-02,  2.3275e-01,  1.5560e-01,\n",
       "          -5.8212e-02, -4.6966e-02, -8.8897e-02,  6.3531e-01, -5.7413e-02,\n",
       "          -4.4482e-02, -2.1070e-01,  2.1130e-02, -2.0953e-02,  2.9690e-01,\n",
       "          -2.3055e-01, -1.1976e-01,  3.5724e-01, -3.7660e-01, -3.1279e-01,\n",
       "          -9.4088e-02,  4.0741e-02, -1.2041e-01,  1.1146e-02, -1.1843e-01,\n",
       "          -1.1988e-01, -2.0048e-01,  3.7020e-01, -5.4336e-03, -2.1060e-01,\n",
       "          -3.5244e-01,  1.9725e-01, -2.8410e-01, -2.1667e-02,  1.1906e-01,\n",
       "          -1.6232e-01,  1.4037e-01, -2.6254e-01, -1.9619e-02,  1.3825e-01,\n",
       "          -1.0288e-01, -3.1256e-01,  2.2326e-01,  4.7886e-01,  2.3181e-01,\n",
       "           3.2818e-01, -1.9606e-02,  8.2409e-02, -4.2025e-01,  3.6256e-02,\n",
       "           5.5572e-02, -1.5449e-01, -4.0418e-02, -6.1560e-02, -1.8169e-01,\n",
       "           2.4517e-01, -3.7002e-02, -3.6960e-01, -8.8844e-02, -2.3780e-01,\n",
       "           1.6417e-01,  1.0817e-01, -1.7400e-01,  2.7917e-01, -2.2443e-01,\n",
       "          -6.6816e-02, -2.3988e-01,  1.5288e-01, -2.1164e-02, -8.4117e-02,\n",
       "           2.0744e-02, -1.7678e-01,  9.1154e-02, -7.9493e-02,  2.3486e-01,\n",
       "           9.4412e-02,  5.9954e-02, -1.3508e-01,  2.5998e-01,  4.7383e-01,\n",
       "           1.8552e-01,  4.0855e-01, -3.3529e-01, -1.3373e-01, -2.2181e-01,\n",
       "          -2.9527e-01, -3.4575e-01, -3.4687e-01,  1.6851e-01,  3.5043e-02,\n",
       "           4.4696e-02,  3.9911e-01, -4.3358e-01,  6.4146e-02, -2.2804e-02,\n",
       "           6.6244e-02,  9.4090e-02, -3.7099e-01,  4.2853e-01,  5.4155e-04,\n",
       "          -2.1277e-01, -1.7115e-01,  2.4853e-01, -3.4741e-01,  6.0674e-02,\n",
       "          -9.0868e-02,  4.6684e-01, -1.0172e-02,  2.2984e-02,  2.6720e-01,\n",
       "          -3.6501e-01,  1.1859e-01,  2.6614e-01,  2.3305e-01, -7.0079e-02,\n",
       "           1.9824e-01,  1.5617e-01,  2.7291e-01, -1.5273e-01,  8.7938e-02,\n",
       "           1.3764e-01, -1.0097e-01,  7.3936e-02,  6.1457e-03, -4.9488e-02,\n",
       "           1.1659e-01, -1.7241e-01,  1.4152e-01, -1.5163e-01,  1.0425e-01]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'pred_boxes': tensor([[[0.5226, 0.5000, 0.5012, 0.4955],\n",
       "          [0.4468, 0.5000, 0.5005, 0.5208],\n",
       "          [0.4665, 0.5000, 0.5041, 0.5053],\n",
       "          [0.5183, 0.4841, 0.4994, 0.4962],\n",
       "          [0.5010, 0.4994, 0.4956, 0.5038],\n",
       "          [0.5018, 0.5091, 0.5000, 0.5001],\n",
       "          [0.5035, 0.5007, 0.5001, 0.4972],\n",
       "          [0.4998, 0.4993, 0.5001, 0.4918],\n",
       "          [0.5001, 0.4853, 0.4986, 0.5001],\n",
       "          [0.4961, 0.4937, 0.4984, 0.5000],\n",
       "          [0.5032, 0.5020, 0.5033, 0.4986],\n",
       "          [0.4774, 0.5053, 0.4980, 0.4859],\n",
       "          [0.5008, 0.5030, 0.4962, 0.5077],\n",
       "          [0.5094, 0.5000, 0.4998, 0.5009],\n",
       "          [0.5095, 0.5000, 0.4941, 0.5251],\n",
       "          [0.4990, 0.4840, 0.4890, 0.5169],\n",
       "          [0.5034, 0.4995, 0.4961, 0.5254],\n",
       "          [0.4965, 0.5161, 0.5023, 0.4759],\n",
       "          [0.4941, 0.5123, 0.4943, 0.4459],\n",
       "          [0.4589, 0.5074, 0.4902, 0.5066],\n",
       "          [0.4923, 0.4997, 0.5021, 0.5038],\n",
       "          [0.5020, 0.4990, 0.4998, 0.4757],\n",
       "          [0.4987, 0.5042, 0.5275, 0.5134],\n",
       "          [0.5207, 0.4793, 0.5047, 0.4888],\n",
       "          [0.5247, 0.5010, 0.4992, 0.5123]],\n",
       " \n",
       "         [[0.5226, 0.5000, 0.5012, 0.4955],\n",
       "          [0.4468, 0.5000, 0.5005, 0.5208],\n",
       "          [0.4665, 0.5000, 0.5041, 0.5053],\n",
       "          [0.5183, 0.4841, 0.4994, 0.4962],\n",
       "          [0.5010, 0.4994, 0.4956, 0.5038],\n",
       "          [0.5018, 0.5091, 0.5000, 0.5001],\n",
       "          [0.5035, 0.5007, 0.5001, 0.4972],\n",
       "          [0.4998, 0.4993, 0.5001, 0.4918],\n",
       "          [0.5001, 0.4853, 0.4986, 0.5001],\n",
       "          [0.4961, 0.4937, 0.4984, 0.5000],\n",
       "          [0.5032, 0.5020, 0.5033, 0.4986],\n",
       "          [0.4774, 0.5053, 0.4980, 0.4859],\n",
       "          [0.5008, 0.5030, 0.4962, 0.5077],\n",
       "          [0.5094, 0.5000, 0.4998, 0.5009],\n",
       "          [0.5095, 0.5000, 0.4941, 0.5251],\n",
       "          [0.4990, 0.4840, 0.4890, 0.5169],\n",
       "          [0.5034, 0.4995, 0.4961, 0.5254],\n",
       "          [0.4965, 0.5161, 0.5023, 0.4759],\n",
       "          [0.4941, 0.5123, 0.4943, 0.4459],\n",
       "          [0.4589, 0.5074, 0.4902, 0.5066],\n",
       "          [0.4923, 0.4997, 0.5021, 0.5038],\n",
       "          [0.5020, 0.4990, 0.4998, 0.4757],\n",
       "          [0.4987, 0.5042, 0.5275, 0.5134],\n",
       "          [0.5207, 0.4793, 0.5047, 0.4888],\n",
       "          [0.5247, 0.5010, 0.4992, 0.5123]]], grad_fn=<SigmoidBackward0>)}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "DSIM",
   "language": "python",
   "name": "dsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
